import argparse
import json
import os

from tqdm import tqdm

from classifier_predict import predict_multi_data, is_report_perfect
from analysis_sample import analysis_llm_data, select_dataset_for_llm
import sys
sys.path.append('..')
from gpt_utils import call_ChatGPT
try:
    from pkl2json import parse_pkl_bug_report
    from plot_utils import plot_llm_dataset_ring
except ImportError:
    # Create dummy functions if modules don't exist
    def parse_pkl_bug_report(args, filename):
        print(f"Warning: pkl2json module not found, skipping {filename}")
    
    def plot_llm_dataset_ring(result_path, max_length, project_list):
        print(f"Warning: plot_utils module not found, skipping plotting")


# 1. è½¬æ¢æ•°æ®ç±»å‹
# 2. é¢„æµ‹bugæŠ¥å‘Šè´¨é‡ï¼Œå¹¶å°†é«˜è´¨é‡æŠ¥å‘Šæ ¼å¼åŒ–ä¸ºgptéœ€è¦çš„æ ¼å¼, é€‰æ‹©ç¬¦åˆé•¿åº¦çš„bugæŠ¥å‘Šï¼Œç»˜å›¾
# 3. è°ƒç”¨GPT
# 4. åˆ†æå®éªŒç»“æœï¼Œ ç»˜å›¾

def parse_run_arguments():
    """è§£æå‚æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--pkl_data_path', default='./mock_data/pickles/',
                        help='åŸå§‹pklæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--json_bug_path',
                        default='./origin_data/',
                        help='pklä¿å­˜æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--bert_result_path', default='./predict_data/bee_tool/', help='bertæ¨¡å‹é¢„æµ‹ç»“æœè·¯å¾„')
    parser.add_argument('--device', default='gpu')
    parser.add_argument('--num_labels', default=3)
    parser.add_argument('--max_length', default=128)
    parser.add_argument('--hidden_dropout_prob', default=0.3)
    parser.add_argument('--model_path', default='../question1/model/bee')
    parser.add_argument('--llm_data_path', default='./llm_dataset')
    parser.add_argument('--report_max_length', default=2000)

    arguments = parser.parse_args()

    return arguments


def transfer_datatype(args):
    """è½¬æ¢æ•°æ®ç±»å‹"""
    # Skip this step since we're using JSON directly
    print("Skipping data transfer - using JSON files directly")
    return


def run_bert_predict(args):
    # é¢„æµ‹ç¼ºé™·å®šä½é¡¹ç›®çš„æ‰€æœ‰é¡¹ç›®çš„ç¼ºé™·æŠ¥å‘Š
    predict_multi_data(args)
    # åˆ†æç»è¿‡berté¢„æµ‹åï¼Œé‡æ„çš„é«˜è´¨é‡ç¼ºé™·æŠ¥å‘Šé•¿åº¦ä¿¡æ¯
    analysis_llm_data(args.bert_result_path)
    # é€‰æ‹©å­—ç¬¦é•¿åº¦å°äºKçš„ç¼ºé™·æŠ¥å‘Š, å°†é€‰ä¸­çš„ç¼ºé™·æŠ¥å‘Šæ ¼å¼åŒ–ä¸ºllméœ€è¦çš„æ ¼å¼
    select_dataset_for_llm(args.bert_result_path, args.llm_data_path, args.report_max_length)
    # plot (skip if not available)
    try:
        plot_llm_dataset_ring(args.bert_result_path, args.report_max_length, project_list)
    except:
        print("Skipping plotting - plot_utils not available")


def load_sample_call_llm(args):
    # éå†æ¯ä¸ªé¡¹ç›®
    args.gen_data_path = './generate_data'  # Add missing variable
    if not os.path.exists(args.gen_data_path):
        os.mkdir(args.gen_data_path)

    for project in project_list:
        # è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œ éå†æ–‡ä»¶
        project_llm_data_path = os.path.join(args.llm_data_path, project)
        project_gen_data_path = os.path.join(args.gen_data_path, project)
        if not os.path.exists(project_gen_data_path):
            os.mkdir(project_gen_data_path)

        # éå†æ¯ä¸ªé¡¹ç›®çš„ç¼ºé™·æŠ¥å‘Šæ–‡ä»¶
        filelist = os.listdir(project_llm_data_path)
        p_bar = tqdm(filelist, total=len(filelist), desc='iter')
        for idx, filename in enumerate(p_bar):
            p_bar.set_description(f"{project}: No.{idx}")
            # è¯»å–ç¼ºé™·æŠ¥å‘Šæ–‡ä»¶
            ext = os.path.splitext(filename)[1]
            if ext == '.txt':
                with open(os.path.join(project_llm_data_path, filename), 'r') as f:
                    bug_report = f.read()
                response = call_ChatGPT(bug_report, model="gpt-3.5-turbo")
                for i in range(5):
                    if response is not None:
                        with open(os.path.join(project_gen_data_path,
                                               os.path.join(filename.split('.')[0], f"_gpt_{i}.json")), 'w') as f:
                            json.dump(response, f)
                        if is_report_perfect(response['choices'][0]['message']['content'], args):
                            break
                    response = call_ChatGPT(bug_report, model="gpt-3.5-turbo")


if __name__ == '__main__':
    # è§£æå‚æ•°
    project_list = ["AspectJ", "Birt", "Eclipse", "JDT", "SWT", "Tomcat"]
    args = parse_run_arguments()
    
    print("=== ChatBR with BEE-tool Implementation ===")
    print(f"Input data: {args.json_bug_path}")
    print(f"Output results: {args.bert_result_path}")
    print(f"LLM dataset: {args.llm_data_path}")
    print(f"Generated improvements: ./generate_data")
    print()
    
    # åŠ è½½æ•°æ®é›†ï¼Œè°ƒç”¨ChatGPT
    transfer_datatype(args)
    run_bert_predict(args)
    load_sample_call_llm(args)
    
    print("\nğŸ‰ ChatBR implementation completed!")
    print("Check the results in:")
    print("- Classification results: ./predict_data/bee_tool/")
    print("- LLM dataset: ./llm_dataset/")
    print("- Generated improvements: ./generate_data/")
